{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5beb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD, NMF, KNNWithZScore, CoClustering\n",
    "from surprise.model_selection import ShuffleSplit\n",
    "from surprise import accuracy\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5254c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo! Data\n",
    "ratings_data = 'K:/Datasets/yahoo/ydata-ymusic-user-song-ratings-meta-v1_0/train_0.txt'\n",
    "ratings = pd.read_csv(ratings_data, sep=\"\\t\", header=None)\n",
    "ratings.columns = ['user_id', 'song_id', 'rating']\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many songs each user has listened to\n",
    "user_counts = ratings.groupby('user_id')['song_id'].count()\n",
    "user_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many users listen to the same song on average\n",
    "song_user = ratings.groupby('song_id')['user_id'].count()\n",
    "song_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd517fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out users that have listened to less than 16 songs\n",
    "flt_users = user_counts[user_counts > 50].index.to_list() # 25%\n",
    "\n",
    "# filter out songs that have less than 200 users\n",
    "flt_songs = song_user[song_user > 150].index.to_list() # 150 users \n",
    "\n",
    "# filter out dataset with user and song id's\n",
    "ratings_flt = ratings[(ratings['user_id'].isin(flt_users)) & (ratings['song_id'].isin(flt_songs))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683dec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10,000 songs with most ratings\n",
    "top_songs = song_user.sort_values(ascending=False)[:10000].index.to_list()\n",
    "# filter for only top 2500 popular songs\n",
    "ratings_flt_pop = ratings_flt[(ratings_flt['song_id'].isin(top_songs))].reset_index(drop=True)\n",
    "\n",
    "# top 10,000 users with most ratings\n",
    "most_ratings_user = user_counts.sort_values(ascending=False)[:10000].index.to_list()\n",
    "ratings_flt_pop = ratings_flt_pop[(ratings_flt_pop['user_id'].isin(most_ratings_user))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = ratings_flt_pop.user_id.nunique()\n",
    "n_items = ratings_flt_pop.song_id.nunique()\n",
    "print('Users: ' + str(n_users), 'Songs: ' + str(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Reader class with rating scale from 1 to 5\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# load a new dataset object with Yahoo ratings\n",
    "data = Dataset.load_from_df(ratings_flt_pop[['user_id', 'song_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010efa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from suprise library website\n",
    "# Source: https://surprise.readthedocs.io/en/stable/FAQ.html\n",
    "\n",
    "# Return precision and recall at k metrics for each user\n",
    "def precision_recall_at_k(predictions, k=20, threshold=3):\n",
    "    \n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "    \n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [SVD(), NMF(), KNNWithZScore(), CoClustering()]\n",
    "test_size = [\"0.25\", \"0.30\", \"0.40\"]\n",
    "n_splits = 5\n",
    "\n",
    "results = {}\n",
    "\n",
    "for algorithm in algos:\n",
    "    algo_dict = {}\n",
    "\n",
    "    rmse_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    \n",
    "    algorithm_name = str(algorithm).split(\".\")[3].split(\" \")[0]\n",
    "    print(f'Algorithm: {algorithm_name}')\n",
    "    print('------------------------------')\n",
    "    \n",
    "    for size in test_size:\n",
    "        test_dict = {}\n",
    "        print(f'\\nTest size: {size}')\n",
    "        print('====================')\n",
    "        \n",
    "        kf = ShuffleSplit(n_splits=n_splits, test_size=float(size), shuffle=True, random_state=42)\n",
    "        rmse_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        \n",
    "        for trainset, testset in kf.split(data):\n",
    "            # training\n",
    "            print('Model is being trained...')\n",
    "            algo = algorithm\n",
    "            algo.fit(trainset) \n",
    "            print('Training Successful\\n')\n",
    "\n",
    "            # testing\n",
    "            print('Model is being tested...')\n",
    "            predictions = algo.test(testset)\n",
    "            result = round(accuracy.rmse(predictions, verbose=False), 4)\n",
    "            print('Testing Successful\\n')\n",
    "            print(f'Testset RMSE is {result}')\n",
    "            rmse_list.append(result)\n",
    "    \n",
    "            # recall and precision @ 20\n",
    "            precisions, recalls = precision_recall_at_k(predictions, k=20, threshold=3)\n",
    "            precision = round(sum(prec for prec in precisions.values()) / len(precisions), 4)\n",
    "            recall = round(sum(rec for rec in recalls.values()) / len(recalls), 4)\n",
    "            \n",
    "            print(f'Precision: {precision}')\n",
    "            print(f'Recall: {recall}')\n",
    "            print('--------------------')\n",
    "            \n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "        \n",
    "        avg_rmse = round(mean(rmse_list), 4)\n",
    "        avg_precision = round(mean(precision_list), 4)\n",
    "        avg_recall = round(mean(recall_list), 4)\n",
    "        \n",
    "        print('********************')\n",
    "        print(f'Mean Precision: {avg_precision}')\n",
    "        print(f'Mean Recall: {avg_recall}')\n",
    "        \n",
    "        test_dict['rmse'] = avg_rmse\n",
    "        test_dict['precision'] = avg_precision\n",
    "        test_dict['recall'] = avg_recall\n",
    "        \n",
    "        algo_dict[f'testset_{size}'] = test_dict\n",
    "    \n",
    "    results[algorithm_name] = algo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of results\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
