{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b06c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD, NMF, KNNWithZScore, CoClustering\n",
    "from surprise.model_selection import ShuffleSplit\n",
    "from surprise import accuracy\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d9391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Triplets Data\n",
    "train_triplets = 'K:/Datasets/EchoNest/Train Triplets/train_triplets.txt'\n",
    "train_ratings = pd.read_csv(train_triplets, sep=\"\\t\", header=None)\n",
    "train_ratings.columns = ['user_id', 'song_id', 'listen_count']\n",
    "\n",
    "# Spotify Metadata \n",
    "song_metadata = pd.read_csv('K:/Notebook Files/spotify_metadata.csv')\n",
    "\n",
    "# get ratings for songs that are in the metadata\n",
    "ratings = train_ratings[train_ratings['song_id'].isin(song_metadata['en_song_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66cad11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1019302.000000\n",
       "mean          45.680929\n",
       "std           55.784434\n",
       "min            1.000000\n",
       "25%           15.000000\n",
       "50%           26.000000\n",
       "75%           53.000000\n",
       "max         4228.000000\n",
       "Name: song_id, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many songs each user has listened to\n",
    "user_counts = ratings.groupby('user_id')['song_id'].count()\n",
    "user_counts.describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f7fad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    354962.000000\n",
       "mean        131.176470\n",
       "std         801.194337\n",
       "min           1.000000\n",
       "25%           4.000000\n",
       "50%          14.000000\n",
       "75%          55.000000\n",
       "max       90476.000000\n",
       "Name: user_id, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many users listen to the same song on average\n",
    "song_user = ratings.groupby('song_id')['user_id'].count()\n",
    "song_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e633e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out users that have listened to less than 16 songs\n",
    "flt_users = user_counts[user_counts > 15].index.to_list() #25%\n",
    "\n",
    "# filter out songs that have less than 200 users\n",
    "flt_songs = song_user[song_user > 14].index.to_list() #50%\n",
    "\n",
    "# filter out dataset with user and song id's\n",
    "ratings_flt = ratings[(ratings['user_id'].isin(flt_users)) & (ratings['song_id'].isin(flt_songs))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d73e72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10000 songs with most ratings\n",
    "top_songs = song_user.sort_values(ascending=False)[:10000].index.to_list()\n",
    "# filter for only top 10000 popular songs\n",
    "ratings_flt_pop = ratings_flt[(ratings_flt['song_id'].isin(top_songs))].reset_index(drop=True)\n",
    "\n",
    "# top 10000 users with most ratings\n",
    "most_ratings_user = user_counts.sort_values(ascending=False)[:10000].index.to_list()\n",
    "# filter for only top 10000 users\n",
    "ratings_flt_pop = ratings_flt_pop[(ratings_flt_pop['user_id'].isin(most_ratings_user))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc680c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning technique \n",
    "bins = [0,1,2,3,4,5,6,7,8,9,ratings_flt_pop['listen_count'].max()]\n",
    "ratings_flt_pop['rating'] = pd.cut(ratings_flt_pop['listen_count'], bins=bins, labels=[1,2,3,4,5,6,7,8,9,10])\n",
    "ratings_flt_pop['rating'] = ratings_flt_pop.rating.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a790fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reader class with rating scale from 1 to 10\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "\n",
    "# load dataset class with ratings \n",
    "data = Dataset.load_from_df(ratings_flt_pop[['user_id', 'song_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2440fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Content Based Recommendations ***\n",
    "\n",
    "top_song_features = song_metadata[(song_metadata['en_song_id'].isin(top_songs))].reset_index(drop=True)\n",
    "\n",
    "def feature_rec(data, song_id):\n",
    "    # get data for specific song\n",
    "    song_artist_data = data[data['en_song_id'] == song_id]\n",
    "    \n",
    "    song = song_artist_data['title'].values[0] # song title\n",
    "    artist = song_artist_data['artist'].values[0] # songs artist\n",
    "    \n",
    "    # find songs with similar audio features\n",
    "    similar_songs = data.copy()\n",
    "    sound_properties = similar_songs.loc[:, data.columns[6:18].to_list()]\n",
    "    similar_songs['similarity'] = cosine_similarity(sound_properties, sound_properties.to_numpy()[song_artist_data.index[0], None]).squeeze()\n",
    "    similar_songs = similar_songs.sort_values(by='similarity', ascending=False)\n",
    "    similar_songs = similar_songs['en_song_id']\n",
    "\n",
    "    return similar_songs.iloc[1:2].to_list()\n",
    "\n",
    "def cb_recommendations(data, user_id):\n",
    "    # get id's for users most listened song\n",
    "    song_count = ratings_flt_pop[ratings_flt_pop['user_id'] == user_id]['song_id'].shape[0]\n",
    "    user_songs = ratings_flt_pop[ratings_flt_pop['user_id'] == user_id].sort_values(by='listen_count', ascending=False)['song_id'][:10]\n",
    "\n",
    "    # append similar song to list\n",
    "    recommendations = []\n",
    "    for song in user_songs:\n",
    "        sim_songs = feature_rec(data, song)\n",
    "        for song in sim_songs:\n",
    "            recommendations.append(song)\n",
    "    return random.sample(recommendations, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d562f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from suprise library website\n",
    "\n",
    "# Return precision and recall at k metrics for each user\n",
    "def hybrid_recommendations(user_id, predictions, n=20):\n",
    "    \n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        if(uid == user_id):\n",
    "            user_est_true[uid].append((round(est, 2), round(true_r, 3), iid))    \n",
    "    \n",
    "    cb_list = cb_recommendations(top_song_features, user_id)\n",
    "    cb_pred = []\n",
    "    for song_id in cb_list:\n",
    "        cb_pred.append(algo.predict(user_id, song_id, r_ui=3))\n",
    "    \n",
    "    i = 0\n",
    "    for tpl in list(user_est_true.values())[0]:\n",
    "        i+=1\n",
    "        if(tpl[2] in (cb_list)):\n",
    "            del user_est_true[user_id][i-1]\n",
    "    \n",
    "    for uid, iid, true_r, est, _ in cb_pred:\n",
    "        # print((round(est, 2), round(true_r, 2)))\n",
    "        user_est_true[uid].insert(random.randint(0, 20), (round(est, 2), round(true_r, 2), iid))    \n",
    "        # user_est_true[uid].append((round(est, 2), round(true_r, 2), iid))  \n",
    "    \n",
    "    user_ratings = []\n",
    "    recommendations = []\n",
    "    for est, true_r, song_id in sorted(list(user_est_true.values())[0], key=lambda x: x[0], reverse=True):\n",
    "        user_ratings.append((est, true_r))\n",
    "        recommendations.append(song_id)\n",
    "    \n",
    "    return user_ratings, recommendations[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1a6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from suprise library website\n",
    "# source: https://surprise.readthedocs.io/en/stable/FAQ.html\n",
    "\n",
    "def precision_recall_at_k_hybrid(user_ratings, k=15, threshold=4):\n",
    "    \n",
    "    for est, true_r in user_ratings:\n",
    "        # How many relevant items\n",
    "        num_rel = sum((true_r >= threshold) for (est, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, true_r) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recall = n_rel_and_rec_k / num_rel if num_rel != 0 else 1\n",
    "        \n",
    "    return round(precision, 2), round(recall, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af5bbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from suprise library website\n",
    "# source: https://surprise.readthedocs.io/en/stable/FAQ.html\n",
    "\n",
    "# Return precision and recall at k metrics for each user\n",
    "def precision_recall_at_k(predictions, k=15, threshold=4):\n",
    "    \n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        \n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        \n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        \n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "    \n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fd795e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Algorithm: SVD\n",
      "------------------------------\n",
      "\n",
      "Test size: 0.30\n",
      "====================\n",
      "Model is being trained...\n",
      "Training Successful\n",
      "\n",
      "Model is being tested...\n",
      "Testing Successful\n",
      "\n",
      "Testset RMSE is 2.0946\n",
      "Precision: 0.7962\n",
      "Recall: 0.2827\n",
      "--------------------\n",
      "Model is being trained...\n",
      "Training Successful\n",
      "\n",
      "Model is being tested...\n",
      "Testing Successful\n",
      "\n",
      "Testset RMSE is 2.101\n",
      "Precision: 0.8182\n",
      "Recall: 0.2802\n",
      "--------------------\n",
      "********************\n",
      "Mean Precision: 0.8072\n",
      "Mean Recall: 0.2814\n",
      "0.6936 0.23879999999999998\n",
      "\n",
      "Algorithm: NMF\n",
      "------------------------------\n",
      "\n",
      "Test size: 0.30\n",
      "====================\n",
      "Model is being trained...\n",
      "Training Successful\n",
      "\n",
      "Model is being tested...\n",
      "Testing Successful\n",
      "\n",
      "Testset RMSE is 2.3052\n",
      "Precision: 0.6306\n",
      "Recall: 0.3092\n",
      "--------------------\n",
      "Model is being trained...\n",
      "Training Successful\n",
      "\n",
      "Model is being tested...\n",
      "Testing Successful\n",
      "\n",
      "Testset RMSE is 2.3177\n",
      "Precision: 0.6405\n",
      "Recall: 0.314\n",
      "--------------------\n",
      "********************\n",
      "Mean Precision: 0.6356\n",
      "Mean Recall: 0.3116\n",
      "0.6731999999999999 0.25880000000000003\n"
     ]
    }
   ],
   "source": [
    "# algos = [SVD(), NMF(), KNNWithZScore(), CoClustering()]\n",
    "# test_size = [\"0.25\", \"0.30\", \"0.40\"]\n",
    "\n",
    "algos = [SVD(), NMF()]\n",
    "test_size = [\"0.30\"]\n",
    "n_splits = 2\n",
    "\n",
    "user_sample = ratings_flt_pop['user_id'].sample(n=25, random_state=10)\n",
    "\n",
    "results = {}\n",
    "hybrid_results = {}\n",
    "\n",
    "for algorithm in algos:\n",
    "    algo_dict = {}\n",
    "\n",
    "    rmse_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    \n",
    "    algorithm_name = str(algorithm).split(\".\")[3].split(\" \")[0]\n",
    "    print(f'\\nAlgorithm: {algorithm_name}')\n",
    "    print('------------------------------')\n",
    "    \n",
    "    for size in test_size:\n",
    "        test_dict = {}\n",
    "        print(f'\\nTest size: {size}')\n",
    "        print('====================')\n",
    "        \n",
    "        kf = ShuffleSplit(n_splits=n_splits, test_size=float(size), shuffle=True, random_state=42)\n",
    "        rmse_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        \n",
    "        for trainset, testset in kf.split(data):\n",
    "            # training\n",
    "            print('Model is being trained...')\n",
    "            algo = algorithm\n",
    "            algo.fit(trainset) \n",
    "            print('Training Successful\\n')\n",
    "\n",
    "            # testing\n",
    "            print('Model is being tested...')\n",
    "            predictions = algo.test(testset)\n",
    "            result = round(accuracy.rmse(predictions, verbose=False), 4)\n",
    "            print('Testing Successful\\n')\n",
    "            print(f'Testset RMSE is {result}')\n",
    "            rmse_list.append(result)\n",
    "            \n",
    "            # recall and precision @ 20\n",
    "            precisions, recalls = precision_recall_at_k(predictions, k=15, threshold=4)\n",
    "            precision = round(sum(prec for prec in precisions.values()) / len(precisions), 4)\n",
    "            recall = round(sum(rec for rec in recalls.values()) / len(recalls), 4)\n",
    "            \n",
    "            print(f'Precision: {precision}')\n",
    "            print(f'Recall: {recall}')\n",
    "            print('--------------------')\n",
    "            \n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "        \n",
    "        avg_rmse = round(mean(rmse_list), 4)\n",
    "        avg_precision = round(mean(precision_list), 4)\n",
    "        avg_recall = round(mean(recall_list), 4)\n",
    "        \n",
    "        print('********************')\n",
    "        print(f'Mean Precision: {avg_precision}')\n",
    "        print(f'Mean Recall: {avg_recall}')\n",
    "        \n",
    "        test_dict['rmse'] = avg_rmse\n",
    "        test_dict['precision'] = avg_precision\n",
    "        test_dict['recall'] = avg_recall\n",
    "        \n",
    "        algo_dict[f'testset_{size}'] = test_dict\n",
    "        \n",
    "    results[algorithm_name] = algo_dict\n",
    "        \n",
    "    # Hybrid Testing\n",
    "    hybrid_metrics = {}\n",
    "\n",
    "    hybrid_precision = []\n",
    "    hybrid_recall = []\n",
    "    for user in user_sample:\n",
    "        user_ratings, recommendations = hybrid_recommendations(user, predictions)\n",
    "        precision, recall = precision_recall_at_k_hybrid(user_ratings)\n",
    "        hybrid_precision.append(precision)\n",
    "        hybrid_recall.append(recall)\n",
    "    \n",
    "    avg_hybrid_precision = np.mean(hybrid_precision)\n",
    "    avg_hybrid_recall = np.mean(hybrid_recall)\n",
    "    \n",
    "    print(avg_hybrid_precision, avg_hybrid_recall)\n",
    "    \n",
    "    hybrid_metrics['precision'] = avg_hybrid_precision\n",
    "    hybrid_metrics['recall'] = avg_hybrid_recall\n",
    "\n",
    "    hybrid_results[algorithm_name] = hybrid_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f65344a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVD': {'testset_0.30': {'rmse': 2.0978, 'precision': 0.8072, 'recall': 0.2814}}, 'NMF': {'testset_0.30': {'rmse': 2.3114, 'precision': 0.6356, 'recall': 0.3116}}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5670758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVD': {'precision': 0.6936, 'recall': 0.23879999999999998}, 'NMF': {'precision': 0.6731999999999999, 'recall': 0.25880000000000003}}\n"
     ]
    }
   ],
   "source": [
    "print(hybrid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bec251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
